根据初始学习率设置的：0.1*(bn*gpu)/256（bn=16，gpu=2,所以warm up之后的初始学习率设置为0.0125）
用Adam优化器 学习率0.0025跑一个epoch(这里epoch用了5000张图)。作为warm up。精度超过0.35[失败了，在sgd上刚开始的时候出错了，梯度爆炸，后来在此基础上又用0.00125的sgd跑了1000张图片]考虑到两个gpu跑的这里的数字要乘以2
之后用sgh优化器，学习率从0.0125开始，用余弦退火的方式，T=65.训练3天3夜.
trian和val基本不变。
