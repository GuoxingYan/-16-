{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import xml.dom.minidom\n",
    "from xml.dom.minidom import Document  \n",
    "import math\n",
    "import codecs\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import random\n",
    "from PIL import Image\n",
    "from glob import glob\n",
    "from collections import Counter\n",
    "import h5py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import argparse\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "#import torchvision\n",
    "#from torchvision import transforms\n",
    "#from torch.utils.data import Dataset\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "#from utils1 import progress_bar\n",
    "#from models.SENet.se_resnet import se_resnet20_v2,se_resnet20\n",
    "from utils.LabelSmooth import LabelSmoothing\n",
    "\n",
    "# from models.preact_resnet import *\n",
    "#from models.preact_resnet_sn import *\n",
    "from models.senet import SENet18,SENet34,SENet101,SENet152\n",
    "from models.senet_sn import SENet34_SN,SENet101_SN,SENet152_SN\n",
    "#多gpu训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def progress_bar(current, total, msg=None):\n",
    "    global last_time, begin_time\n",
    "    if current == 0:\n",
    "        begin_time = time.time()  # Reset for new bar.\n",
    "\n",
    "    cur_len = int(TOTAL_BAR_LENGTH*current/total)\n",
    "    rest_len = int(TOTAL_BAR_LENGTH - cur_len) - 1\n",
    "\n",
    "    sys.stdout.write(' [')\n",
    "    for i in range(cur_len):\n",
    "        sys.stdout.write('=')\n",
    "    sys.stdout.write('>')\n",
    "    for i in range(rest_len):\n",
    "        sys.stdout.write('.')\n",
    "    sys.stdout.write(']')\n",
    "\n",
    "    cur_time = time.time()\n",
    "    step_time = cur_time - last_time\n",
    "    last_time = cur_time\n",
    "    tot_time = cur_time - begin_time\n",
    "\n",
    "    L = []\n",
    "    L.append('  Step: %s' % format_time(step_time))\n",
    "    L.append(' | Tot: %s' % format_time(tot_time))\n",
    "    if msg:\n",
    "        L.append(' | ' + msg)\n",
    "\n",
    "    msg = ''.join(L)\n",
    "    sys.stdout.write(msg)\n",
    "    for i in range(term_width-int(TOTAL_BAR_LENGTH)-len(msg)-3):\n",
    "        sys.stdout.write(' ')\n",
    "\n",
    "    # Go back to the center of the bar.\n",
    "    for i in range(term_width-int(TOTAL_BAR_LENGTH/2)+2):\n",
    "        sys.stdout.write('\\b')\n",
    "    sys.stdout.write(' %d/%d ' % (current+1, total))\n",
    "\n",
    "    if current < total-1:\n",
    "        sys.stdout.write('\\r')\n",
    "    else:\n",
    "        sys.stdout.write('\\n')\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imgaug import augmenters as iaa\n",
    "\n",
    "seq = iaa.Sequential([iaa.GaussianBlur(sigma=(0.25, 0.75)),\n",
    "#     iaa.Fliplr(0.5),#有向右转，向左转，不能这么做\n",
    "    iaa.Flipud(0.2),\n",
    "#     iaa.Sometimes(0.1,iaa.Rot90((1, 3))),#90,180,270\n",
    "    iaa.Affine(\n",
    "        scale={\"x\": (0.95, 1.05), \"y\": (0.95, 1.05)},\n",
    "        translate_percent={\"x\": (-0.0, 0.0), \"y\": (-0.01, 0.01)},\n",
    "        rotate=(-1.1, 1.1),\n",
    "        shear=(-1.1, 1.1)\n",
    "    ),\n",
    "#     iaa.Multiply((0.9, 1.1), per_channel=False),\n",
    "    iaa.SomeOf(1 ,[\n",
    "    iaa.PiecewiseAffine(scale=(0.01,0.02)),#0.01   \n",
    "    iaa.PerspectiveTransform(scale=(0.01, 0.03))], random_order=True)# 透视变化，值越大，变化越明显\n",
    "]) # apply augmenters in random modelorder\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator():\n",
    "    def __init__(self,\n",
    "                 filepath='./data/',\n",
    "                 batch_size=8,\n",
    "                 datatype='train',\n",
    "                 split=0.1):\n",
    "        train_file = h5py.File(os.path.join(filepath, 'hld_train.h5'), 'r')     \n",
    "        self.train_X = train_file['images']\n",
    "        self.train_Y = train_file['labels']\n",
    "        # 统计每一个数据集的数量\n",
    "        self.num_train = self.train_Y.shape[0]\n",
    "        # 按照batch_size进行（分组）采样\n",
    "        # 得到每一个分组的索引 [0, 8, 16, 24, ...]\n",
    "        #num_groups = int((self.num_train + self.num_val) / batch_size)\n",
    "        num_groups = int((self.num_train) / batch_size)\n",
    "        self.indices = np.arange(num_groups) * batch_size#这里其实少了最后面的一个batch，但是就不用考虑train和val中间的情况\n",
    "        np.random.seed(3)\n",
    "        np.random.shuffle(self.indices)\n",
    "        # 这里只选择总数的后1/10作为验证集\n",
    "        # 其余的作为训练集 cudnn.benchmark = True\n",
    "        split = int(num_groups * split)\n",
    "        split = -split if split else None\n",
    "        self.datatype=datatype\n",
    "        if datatype == 'train':\n",
    "            self.indices = self.indices[:split]\n",
    "        else:\n",
    "            self.indices = self.indices[split:]\n",
    "        #count是指示的总数/batchsize\n",
    "        self.count = self.indices.size\n",
    "        self.batch_size = batch_size\n",
    "        self.index = 0\n",
    "\n",
    "    def next_batch(self):\n",
    "        idx = self.indices[self.index]\n",
    "\n",
    "        images = self.train_X[idx:idx + self.batch_size,:,:,:]\n",
    "\n",
    "        labels = self.train_Y[idx:idx + self.batch_size]\n",
    "\n",
    "        self.index += 1\n",
    "        if self.index >= self.count:\n",
    "            self.index = 0\n",
    "            np.random.shuffle(self.indices)\n",
    "\n",
    "        images = np.asarray(images, dtype=np.float32)\n",
    "        labels = np.asarray(labels, dtype=np.float32)\n",
    "\n",
    "        if self.datatype== 'train':\n",
    "            return seq.augment_images(images), labels\n",
    "        else:\n",
    "            return images, labels\n",
    "\n",
    "    def __next__(self):\n",
    "        return self.next_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_optimizer_lr(optimizer, lr):\n",
    "    # callback to set the learning rate in an optimizer, without rebuilding the whole optimizer\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    return optimizer\n",
    "\n",
    "def sgdr(period, batch_idx):\n",
    "    # returns normalised anytime sgdr schedule given period and batch_idx\n",
    "    # best performing settings reported in paper are T_0 = 10, T_mult=2\n",
    "    # so always use T_mult=2\n",
    "    batch_idx = float(batch_idx)\n",
    "    restart_period = period\n",
    "    while batch_idx/restart_period > 1.:\n",
    "        batch_idx = batch_idx - restart_period\n",
    "        restart_period = restart_period * 2.\n",
    "\n",
    "    radians = math.pi*(batch_idx/restart_period)\n",
    "    return 0.5*(1.0 + math.cos(radians))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, num_epochs=25):\n",
    "\n",
    "    since = time.time() \n",
    "\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        #global optimizer\n",
    "        #开始第几次循环\n",
    "        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
    "        print('-' * 10)\n",
    "        \n",
    "\n",
    "        # Each epoch has a training and validation phase!!!\n",
    "        for phase in ['train','val']:\n",
    "            #根据phase不同，将读入的data不同，然后传入\n",
    "            if phase == 'train':\n",
    "                data = trian_data\n",
    "                #scheduler.step()\n",
    "                model.train(True)  # Set model to training mode\n",
    "            else:\n",
    "                data =val_data\n",
    "                model.train(False)  # Set model to evaluate mode\n",
    "                #model.eval() #等效于model.train(False)仅仅当模型中有Dropout和BatchNorm是才会有影响。\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            print_trainloss=0.0\n",
    "            print_traincorrects=0.0\n",
    "            total = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            #Iter=int(data.s1.shape[0]/float(data.batch_size))\n",
    "###输入前16000，验证下效果              \n",
    "            #Iter=1000\n",
    "            Iter=int(data.count)\n",
    "    \n",
    "     \n",
    "            start_batch_idx=Iter*epoch\n",
    "            \n",
    "            trainfalse = {x: 0 for x in np.arange(17)} \n",
    "            valfalse = {x: 0 for x in np.arange(17)} \n",
    "            lr_period = args.lr_period*Iter\n",
    "            for i in (range(Iter)):#用1.6w张图片看下效果\n",
    "                # get the inputs   \n",
    "                inputs, labels = data.next_batch()#迭代器 \n",
    "                inputs=np.ascontiguousarray(inputs, dtype=np.float32)\n",
    "                # wrap them in Variable\n",
    "                if use_gpu:#np_>FloatTensor_>Variable\n",
    "                    inputs = Variable((torch.from_numpy(inputs)).float().permute(0, 3, 1, 2).cuda())#输入必须是float N C H W\n",
    "                    #inputs = train_transform(inputs)\n",
    "                    labels = Variable((torch.from_numpy(labels)).long().cuda())#label必须是long\n",
    "                else:\n",
    "                    inputs, labels = Variable(torch.from_numpy(inputs).permute(0, 3, 1, 2)), Variable(torch.from_numpy(labels).long())\n",
    "\n",
    "                total += labels.size(0)\n",
    "                global_step = i+start_batch_idx\n",
    "                \n",
    "                batch_lr = args.lr*sgdr(lr_period, global_step)\n",
    "                lr_trace.append(batch_lr)\n",
    "                optimizer = set_optimizer_lr(optimizer, batch_lr)\n",
    "                # zero the parameter gradients 因为本身是累加的   \n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward  CE\n",
    "                outputs = model(inputs)\n",
    "                labels =labels.argmax(dim=1)#CE默认不支持one-hot编码\n",
    "                _, preds = torch.max(outputs.data, 1)#这里已经转成了\n",
    "                loss = criterion(outputs, labels)#CE默认不支持one-hot编码\n",
    "                \n",
    "                # backward + optimize only if in training phase\n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    optimizer.step() \n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item()\n",
    "                print_trainloss += loss.item()\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "                print_traincorrects += torch.sum(preds == labels.data)\n",
    "                \n",
    "                ###########################\n",
    "                falselenth=len(labels.data[np.where((preds != labels.data).cpu())])\n",
    "                if phase=='train':\n",
    "                    for j in range(falselenth):\n",
    "                         trainfalse[labels.data[np.where((preds != labels.data).cpu())].cpu().numpy()[j]]+=1\n",
    "                if phase=='val':\n",
    "                    for k in range(falselenth):\n",
    "                         valfalse[labels.data[np.where((preds != labels.data).cpu())].cpu().numpy()[k]]+=1                \n",
    "                \n",
    "               # print_iter_train=1000 #每1000输出一次train的loss和acc\n",
    "\n",
    "                #if i%print_iter_train==0 and i>0 and phase=='train':\n",
    "                 #   print('\\r{} Loss: {:.4f} Acc: {:.4f}'.format(phase,print_trainloss/(print_iter_train*data.batch_size), float(print_traincorrects)/(data.batch_size*print_iter_train)))\n",
    "                  #  print_traincorrects=0.0\n",
    "                  #  print_trainloss=0.0   \n",
    "                if i>0 and phase=='train':    \n",
    "                    progress_bar(i, Iter, 'Loss:%.2f| Acc: %.2f%%(%d/%d) | LR:%.4f'\n",
    "            % (running_loss/(i+1), 100.*float(running_corrects)/total, running_corrects, total, batch_lr))\n",
    "\n",
    "            epoch_loss = float(running_loss) / (Iter*data.batch_size)\n",
    "            epoch_acc = float(running_corrects) / (Iter*data.batch_size)\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "                  \n",
    "            if phase=='train':\n",
    "                print('TrainFalse: {}'.format(trainfalse))\n",
    "            if phase=='val':\n",
    "                print('ValFalse: {}'.format(valfalse))\n",
    "                \n",
    "            save_path=save_root+args.model\n",
    "            if not os.path.isdir(save_path): \n",
    "                os.mkdir(save_path)\n",
    "\n",
    "            torch.save(model.state_dict(),save_path+'/'+args.model+'_{0}.pth'.format(epoch+1))\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--lr_period LR_PERIOD] [--lr LR]\n",
      "                             [--model MODEL]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /run/user/1000/jupyter/kernel-87d51ecd-930b-4c66-97bc-8be0c0e22f0d.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zj/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py:3275: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "###############################################\n",
    "parser = argparse.ArgumentParser(description='PyTorch LCZ Training')\n",
    "parser.add_argument('--lr_period', default=10, type=float, help='learning rate schedule restart period')\n",
    "parser.add_argument('--lr', default=0.001, type=float, help='learning rate')#0.02\n",
    "parser.add_argument('--model', '-s', default='SENet34', help='saves state_dict on every epoch (for resuming best performing model and saving it)')\n",
    "\n",
    "save_root='/home/zj/senetial/save_models/'\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "\n",
    "device_ids = [0]\n",
    "use_gpu = torch.cuda.is_available()\n",
    "lr_trace = []\n",
    "#定义网络\n",
    "\n",
    "#model = PreActResNet18_SN()\n",
    "#model =SENet101_SN()\n",
    "model =SENet34()\n",
    "\n",
    "if use_gpu and len(device_ids)>1:#多gpu训练\n",
    "    model = model.cuda(device_ids[0])\n",
    "    model = nn.DataParallel(model, device_ids=device_ids)\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "if use_gpu and len(device_ids)==1:#单gpu训练\n",
    "    model = model.cuda()\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "print(model)\n",
    "\n",
    "#model.load_state_dict(torch.load('/home/zj/senetial/save_models/SENet101_SN/ori/SENet101_SN_29.pth'))\n",
    "#定义损失函数\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#criterion = LabelSmoothing(size=17,smoothing=0.1)\n",
    "criterion.cuda()\n",
    "#定义优化器\n",
    "optimizer = optim.Adam(model.parameters(), lr=args.lr, momentum=0.8,weight_decay=5e-4)\n",
    "\n",
    "#定义trian的数据和val的数据\n",
    "print('Preparing data..')\n",
    "trian_data = Generator(datatype='train',batch_size=32,split=0.2)\n",
    "val_data = Generator(datatype='val',batch_size=32,split=0.2)\n",
    "\n",
    "model_ft = train_model(model=model,\n",
    "                           criterion=criterion,\n",
    "                           optimizer=optimizer,\n",
    "                           num_epochs=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
